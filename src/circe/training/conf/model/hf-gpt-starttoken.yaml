model_class: circe.models.gpt.hfGPT_starttoken.GPTLMModel
n_positions: 901
vocab_size: 1024 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
n_layer: 6
n_head: 4
n_embd: 256
dropout: 0.2
bias: false # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
block_size: 900 # for inference, 1 minus due to start token
lr: 1e-3
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
