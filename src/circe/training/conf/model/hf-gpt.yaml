model_class: circe.models.gpt.hfGPT.GPTLMModel
n_positions: 512
vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
n_layer: 8
n_head: 8
n_embd: 384
dropout: 0.2
bias: false # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
lr: 1e-3
weight_decay: 1e-1
beta1: 0.9
beta2: 0.95
block_size: 128